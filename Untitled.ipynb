{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "493e3620-3992-46e7-963f-fbff6a0d804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os.path\n",
    "import string\n",
    "import math\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa0b6c6-fde6-4538-91e4-f38b3a541972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['吃了没', 'Have you eaten?', '[sound:azure-ce53eaba-5fc70900-cf7031ea-f6ccda7a-bb7fd84b.mp3]', '[sound:azure-b93e2c66-4158cf8d-8b076261-805e9524-0ed9bd34.mp3]', 'chī le méi'], ['记账。', 'Keep accounts.', '[sound:azure-0bf79036-f498235c-fe549886-74bd7af7-1b6d957c.mp3]', '[sound:azure-c1308ba5-4405dc0f-ea2c8bde-5313312e-fa4ad7db.mp3]', 'jìzhàng 。'], ['可怜!', 'How unfortunate!', '[sound:azure-3f91d44f-d97930fb-0ddcddb3-8719a6b9-a428af00.mp3]', '[sound:azure-4f37e5b4-05e8e3e4-467bb531-cfce0c11-c566c7a7.mp3]', 'kělián !']]\n",
      "[['吃了没', 'chī le méi', 'Have you eaten?', '[sound:azure-ce53eaba-5fc70900-cf7031ea-f6ccda7a-bb7fd84b.mp3]'], ['记账。', 'jìzhàng 。', 'Keep accounts.', '[sound:azure-0bf79036-f498235c-fe549886-74bd7af7-1b6d957c.mp3]'], ['可怜!', 'kělián !', 'How unfortunate!', '[sound:azure-3f91d44f-d97930fb-0ddcddb3-8719a6b9-a428af00.mp3]']]\n"
     ]
    }
   ],
   "source": [
    "with open('LTL Mandarin Chinese Deck Level 1 - Short.txt', 'r', encoding='utf-8-sig') as readFile:\n",
    "    reader = csv.reader(readFile, delimiter='\\t')\n",
    "    lines = list(reader)\n",
    "    \n",
    "print(lines[0:3])\n",
    "    \n",
    "for i in range(len(lines)): lines[i].pop(3)\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    \n",
    "    tmp = [['0']*3]*len(lines)\n",
    "    \n",
    "    for j in range(0, 3):\n",
    "        tmp[i][j] = lines[i][j + 1]\n",
    "    lines[i][1] = tmp[i][2]\n",
    "    lines[i][2] = tmp[i][0]\n",
    "    lines[i][3] = tmp[i][1]\n",
    "\n",
    "print(lines[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9214b72-2307-4ef1-8b48-1276eacc09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LTL Mandarin Chinese Level 1.txt', 'w', encoding='utf-8-sig') as writeFile:\n",
    "    writer = csv.writer(writeFile, delimiter ='\\t')\n",
    "    writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "499cdc3f-67a6-4bfe-9dfe-69103e09dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique character count: 1293\n",
      "from 1998 sentences\n",
      "-------------------\n",
      "Official HSK character count by level: 178, 171, 275, 452, 636, 924\n",
      "Unique characters presented by level: [177. 158. 230. 290. 236. 137.]\n",
      "Cumulative characters exposed by level: [6049. 1525. 1070.  776.  448.  199.]\n",
      "1228.0\n"
     ]
    }
   ],
   "source": [
    "cn_set = [lines[i][0] for i in range(len(lines))]\n",
    "\n",
    "\n",
    "sent_level = np.zeros(len(lines))\n",
    "hsk_chars = np.zeros(1)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    \n",
    "    if i == 1:\n",
    "        with open('HSK\\HSK1.txt', 'r', encoding = 'utf-8-sig') as readFile:\n",
    "            reader = csv.reader(readFile)\n",
    "            hsk_chars = list(reader)   \n",
    "    else:\n",
    "        with open(('HSK\\HSK' + str(i) + '.txt'), 'r', encoding = 'utf-8-sig') as readFile:\n",
    "            reader = csv.reader(readFile)\n",
    "            hsk_chars = np.append(hsk_chars, list(reader))\n",
    "\n",
    "sent_level = np.zeros(6)\n",
    "sent_level_exp = np.zeros(6)\n",
    "\n",
    "all_char = {}\n",
    "\n",
    "for i in range(len(cn_set)):\n",
    "    \n",
    "    cn_set_spl = list(cn_set[i].replace(\" \", \"\").replace(\".\", \"\").replace(\"“\", \"\").replace(\"”\", \"\").replace(\"？\", \"\").replace(\"！\", \"\").replace(\"。\", \"\").replace(\"一\", \"\").replace(\"，\", \"\").replace(\"…\", \"\"))\n",
    "    \n",
    "    for j in range(len(cn_set_spl)):\n",
    "        if cn_set_spl[j] in all_char:\n",
    "            all_char[cn_set_spl[j]] = all_char[cn_set_spl[j]] + 1\n",
    "        else:\n",
    "            all_char[cn_set_spl[j]] = 1\n",
    "            \n",
    "            if cn_set_spl[j] in hsk_chars[0]:\n",
    "                sent_level[0] = sent_level[0] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[1]:\n",
    "                sent_level[1] = sent_level[1] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[2]:\n",
    "                sent_level[2] = sent_level[2] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[3]:\n",
    "                sent_level[3] = sent_level[3] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[4]:\n",
    "                sent_level[4] = sent_level[4] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[5]:\n",
    "                sent_level[5] = sent_level[5] + 1\n",
    "\n",
    "        if cn_set_spl[j] in hsk_chars[0]:\n",
    "            sent_level_exp[0] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[1]:\n",
    "            sent_level_exp[1] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[2]:\n",
    "            sent_level_exp[2] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[3]:\n",
    "            sent_level_exp[3] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[4]:\n",
    "            sent_level_exp[4] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[5]:\n",
    "            sent_level_exp[5] += 1\n",
    "        \n",
    "            \n",
    "sorted_char = sorted(all_char.items(), key = lambda x: x[1], reverse = True)\n",
    "print('Unique character count:', len(sorted_char))\n",
    "print('from', len(cn_set), \"sentences\")\n",
    "print('-------------------')\n",
    "#for i in range(len(sorted_char)): print(sorted_char[i][0], sorted_char[i][1])\n",
    "print('Official HSK character count by level: 178, 171, 275, 452, 636, 924')\n",
    "print('Unique characters presented by level:', sent_level)\n",
    "print('Cumulative characters exposed by level:', sent_level_exp)\n",
    "print(np.sum(sent_level))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b28b0-c85e-4501-b870-851f4b1dbc52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9426a70f-fe2b-44d2-9f0f-4af235bd8839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello!', 'Nǐ hǎo!', '你好！', '[sound:tmp1cctcn.mp3]', ''], ['What are you saying?', 'Nǐ shuō shénme?', '你说什么？', '[sound:tmp4tzxbu.mp3]', ''], ['What did you do?', 'nǐ zuò le shénme ?', '你做了什么？', '[sound:333012.mp3]', '']]\n",
      "[['Hello!', 'Nǐ hǎo!', '你好！', '[sound:tmp1cctcn.mp3]'], ['What are you saying?', 'Nǐ shuō shénme?', '你说什么？', '[sound:tmp4tzxbu.mp3]'], ['What did you do?', 'nǐ zuò le shénme ?', '你做了什么？', '[sound:333012.mp3]']]\n",
      "[['你好！', 'Nǐ hǎo!', 'Hello!', '[sound:tmp1cctcn.mp3]'], ['你说什么？', 'Nǐ shuō shénme?', 'What are you saying?', '[sound:tmp4tzxbu.mp3]'], ['你做了什么？', 'nǐ zuò le shénme ?', 'What did you do?', '[sound:333012.mp3]']]\n"
     ]
    }
   ],
   "source": [
    "with open('SpoonFedChinese.txt', 'r', encoding='utf-8-sig') as readFile:\n",
    "    reader = csv.reader(readFile, delimiter='\\t')\n",
    "    linespoon = list(reader)\n",
    "    \n",
    "print(linespoon[0:3])\n",
    "\n",
    "for i in range(len(linespoon)): linespoon[i].pop(4)\n",
    "\n",
    "print(linespoon[0:3])\n",
    "\n",
    "\n",
    "for i in range(len(linespoon)):\n",
    "    \n",
    "    tmp = [['0']*3]*len(linespoon)\n",
    "    \n",
    "    tmp[i][0] = linespoon[i][0]\n",
    "    tmp[i][1] = linespoon[i][2]\n",
    "    \n",
    "    linespoon[i][0] = tmp[i][1]\n",
    "    linespoon[i][2] = tmp[i][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(linespoon[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ad22c47-f4a5-4aeb-8a84-e7b2bd128bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Spoonfed Chinese.txt', 'w', encoding='utf-8-sig') as writeFile:\n",
    "    writer = csv.writer(writeFile, delimiter ='\\t')\n",
    "    writer.writerows(linespoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f91616-01a8-4883-8f6d-17dd1bdffa26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique character count: 2874\n",
      "from 8142 sentences\n",
      "-------------------\n",
      "Official HSK character count by level: 178, 171, 275, 452, 636, 924\n",
      "Unique characters presented by level: [177. 171. 273. 448. 617. 730.]\n",
      "Cumulative characters exposed by level: [34811.  9784.  8350.  7024.  4770.  2374.]\n",
      "2416.0\n"
     ]
    }
   ],
   "source": [
    "lines = linespoon\n",
    "cn_set = [lines[i][0] for i in range(len(lines))]\n",
    "\n",
    "\n",
    "sent_level = np.zeros(len(lines))\n",
    "hsk_chars = np.zeros(1)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    \n",
    "    if i == 1:\n",
    "        with open('HSK\\HSK1.txt', 'r', encoding = 'utf-8-sig') as readFile:\n",
    "            reader = csv.reader(readFile)\n",
    "            hsk_chars = list(reader)   \n",
    "    else:\n",
    "        with open(('HSK\\HSK' + str(i) + '.txt'), 'r', encoding = 'utf-8-sig') as readFile:\n",
    "            reader = csv.reader(readFile)\n",
    "            hsk_chars = np.append(hsk_chars, list(reader))\n",
    "\n",
    "sent_level = np.zeros(6)\n",
    "sent_level_exp = np.zeros(6)\n",
    "\n",
    "all_char = {}\n",
    "\n",
    "for i in range(len(cn_set)):\n",
    "    \n",
    "    cn_set_spl = list(cn_set[i].replace(\" \", \"\").replace(\".\", \"\").replace(\"“\", \"\").replace(\"”\", \"\").replace(\"？\", \"\").replace(\"！\", \"\").replace(\"。\", \"\").replace(\"一\", \"\").replace(\"，\", \"\").replace(\"…\", \"\"))\n",
    "    \n",
    "    for j in range(len(cn_set_spl)):\n",
    "        if cn_set_spl[j] in all_char:\n",
    "            all_char[cn_set_spl[j]] = all_char[cn_set_spl[j]] + 1\n",
    "        else:\n",
    "            all_char[cn_set_spl[j]] = 1\n",
    "            \n",
    "            if cn_set_spl[j] in hsk_chars[0]:\n",
    "                sent_level[0] = sent_level[0] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[1]:\n",
    "                sent_level[1] = sent_level[1] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[2]:\n",
    "                sent_level[2] = sent_level[2] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[3]:\n",
    "                sent_level[3] = sent_level[3] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[4]:\n",
    "                sent_level[4] = sent_level[4] + 1\n",
    "\n",
    "            elif cn_set_spl[j] in hsk_chars[5]:\n",
    "                sent_level[5] = sent_level[5] + 1\n",
    "\n",
    "        if cn_set_spl[j] in hsk_chars[0]:\n",
    "            sent_level_exp[0] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[1]:\n",
    "            sent_level_exp[1] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[2]:\n",
    "            sent_level_exp[2] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[3]:\n",
    "            sent_level_exp[3] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[4]:\n",
    "            sent_level_exp[4] += 1\n",
    "\n",
    "        elif cn_set_spl[j] in hsk_chars[5]:\n",
    "            sent_level_exp[5] += 1\n",
    "        \n",
    "            \n",
    "sorted_char = sorted(all_char.items(), key = lambda x: x[1], reverse = True)\n",
    "print('Unique character count:', len(sorted_char))\n",
    "print('from', len(cn_set), \"sentences\")\n",
    "print('-------------------')\n",
    "#for i in range(len(sorted_char)): print(sorted_char[i][0], sorted_char[i][1])\n",
    "print('Official HSK character count by level: 178, 171, 275, 452, 636, 924')\n",
    "print('Unique characters presented by level:', sent_level)\n",
    "print('Cumulative characters exposed by level:', sent_level_exp)\n",
    "print(np.sum(sent_level))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "253e59fa-0348-4731-a0bb-e7c67f50fb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16284\n",
      "[]\n",
      "0\n",
      "[['你好！', 'Nǐ hǎo!', 'Hello!', '[sound:tmp1cctcn.mp3]'], ['你说什么？', 'Nǐ shuō shénme?', 'What are you saying?', '[sound:tmp4tzxbu.mp3]'], ['你做了什么？', 'nǐ zuò le shénme ?', 'What did you do?', '[sound:333012.mp3]'], ['我会做。', 'Wǒ huì zuò.', 'I know how to do it.', '[sound:G009-03.mp3]']]\n",
      "8142\n"
     ]
    }
   ],
   "source": [
    "with open('Spoonfed Chinese.txt', 'r', encoding='utf-8-sig') as readFile:\n",
    "    reader = csv.reader(readFile, delimiter='\\t')\n",
    "    lines  = list(reader)\n",
    "    \n",
    "print(len(lines))\n",
    "\n",
    "print(lines[1])\n",
    "print(len(lines[1]))\n",
    "\n",
    "j = 0\n",
    "while j <= len(lines):\n",
    "    \n",
    "    if len(lines[j]) == 0:\n",
    "        del(lines[j])\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "print(lines[0:4])\n",
    "print(len(lines))  \n",
    "nor_set = [lines[i][2] for i in range(len(lines))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f67c51-b41b-417d-bf42-74e73155d940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word count: 6667\n",
      "from 8142 sentences\n"
     ]
    }
   ],
   "source": [
    "all_words = {}\n",
    "\n",
    "for i in range(len(nor_set)):\n",
    "    \n",
    "    nor_low_spl = nor_set[i].replace('.', '').replace('?', '').replace('-', '').replace('!', '').replace('(<--norsk)', '').replace('/', '').replace('(pronoun)', '').replace('(<--english)', '').replace(',', '').replace('maria', '').replace('malin', '').replace('halvor', '').replace('billy', '').replace('hannah', '').replace('bob', '').replace('bjorn', '').replace('pisa', '').replace('jim', '').replace('10', '').replace('15', '').replace('19', '').replace('20', '').replace('50', '').replace('70', '').lower().split()\n",
    "    \n",
    "    for j in range(len(nor_low_spl)):\n",
    "        if nor_low_spl[j] in all_words:\n",
    "            all_words[nor_low_spl[j]] = all_words[nor_low_spl[j]] + 1\n",
    "        else:\n",
    "            all_words[nor_low_spl[j]] = 1\n",
    "\n",
    "sorted_words = sorted(all_words.items(), key = lambda x: x[1], reverse = True)\n",
    "print('word count:', len(sorted_words))\n",
    "print('from', len(nor_set), 'sentences')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
